{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckduckgo_search\n",
    "import ollama\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_search(query):\n",
    "    logging.info(\"Performing web search...\")\n",
    "    ddg = duckduckgo_search.DDGS()\n",
    "    results = ddg.text(query)\n",
    "    search_content = \" \".join([result['body'] for result in results if 'body' in result])\n",
    "    logging.info(\"Web search completed.\")\n",
    "    return search_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_medical_query_llm(query, model_name):\n",
    "    logging.info(\"Determining if the query is medical...\")\n",
    "    messages = [{\"role\": \"system\", \"content\": \"Determine if the following query is medical: {query}. Respond with 'Yes' or 'No'.\"},\n",
    "                {\"role\": \"user\", \"content\": query}]\n",
    "    response = ollama.chat(model=model_name, messages=messages)\n",
    "    is_medical = response.model_dump()[\"message\"][\"content\"].strip().lower() == \"yes\"\n",
    "    logging.info(f\"Query is medical: {is_medical}\")\n",
    "    return is_medical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_context_to_answer(query, conversation_history, model_name):\n",
    "    logging.info(\"Checking if the chat history has context to answer the question...\")\n",
    "    messages = conversation_history + [{\"role\": \"user\", \"content\": f\"Based on the previous conversation, can you answer: {query}? Respond with 'Yes' or 'No'.\"}]\n",
    "    response = ollama.chat(model=model_name, messages=messages)\n",
    "    can_answer = response.model_dump()[\"message\"][\"content\"].strip().lower() == \"yes\"\n",
    "    logging.info(f\"Chat history has context to answer the question: {can_answer}\")\n",
    "    return can_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, conversation_history, context_message=None):\n",
    "    logging.info(\"Generating response...\")\n",
    "    model_name = \"llama3\"  # Replace with the actual model name\n",
    "\n",
    "    if not is_medical_query_llm(query, model_name):\n",
    "        return \"This chatbot only handles medical-related queries. Please ask a medical question.\"\n",
    "\n",
    "    # Add the context message if provided\n",
    "    if context_message:\n",
    "        conversation_history = [{\"role\": \"system\", \"content\": context_message}] + conversation_history\n",
    "\n",
    "    if not has_context_to_answer(query, conversation_history, model_name):\n",
    "        search_content = web_search(query)\n",
    "        conversation_history.append({\"role\": \"system\", \"content\": search_content})\n",
    "\n",
    "    # Generate the final response\n",
    "    messages = conversation_history + [{\"role\": \"user\", \"content\": query}]\n",
    "    response = ollama.chat(model=model_name, messages=messages)\n",
    "    text_response = response.model_dump()[\"message\"][\"content\"]\n",
    "    logging.info(\"Response generated.\")\n",
    "    return text_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    conversation_history = []\n",
    "    context_message = (\"As a medical chatbot, I am here to provide accurate and reliable information \"\n",
    "                       \"exclusively about medical needs. It is very important to check the previous search context \"\n",
    "                       \"to determine if new information is required. If more information is needed, the chatbot should \"\n",
    "                       \"respond with: 'I need more information.'\")\n",
    "\n",
    "    while True:\n",
    "        user_query = input(\"You: \")\n",
    "        if user_query.lower() in [\"exit\", \"quit\"]:\n",
    "            logging.info(\"Exiting...\")\n",
    "            break\n",
    "        response = generate_response(user_query, conversation_history, context_message)\n",
    "        print(f\"Chatbot: {response}\")\n",
    "        conversation_history.append({\"role\": \"user\", \"content\": user_query})\n",
    "        conversation_history.append({\"role\": \"assistant\", \"content\": response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-22 18:42:33,578 - INFO - Starting chatbot...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    logging.info(\"Starting chatbot...\")\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MedAIProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
